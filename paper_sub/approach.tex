\section{Our Approach}
%\label{sec:approach}

%\xj{Explain more details of this new representation: add a figure. and more constraints? say, there are at most four points on each image? Split them to upper/lower parts?}

Under the Manhattan world assumption \cite{coughlan1999manhattan}, a 3D room can be typically modeled as a cube represented by eight corners.
The room layout in a single view is the projection of the cube, while only part of it can be visible, as shown in Fig. \ref{fig:representation}(a-c). 
Intuitively, there should be 0 to 4 room corners visible in a single-view image. 
%
Previous DNN-based techniques of room layout estimation typically represent the room layout as a segmentation of semantic surfaces including walls, ceilings and floors~\cite{dasgupta2016delay} or as the semantic boundaries or intersection points among them \cite{ren2016coarse,zhao2017physics,LeeRoomNet17}. 
%
These representations have proved valid and reasonable. 
However, they all carry too much redundant predictions that make the combination inconvenient under the circumstances of multi-view layout estimation. Taking the keypoints representation in \cite{LeeRoomNet17} as an example, they need to predict all 6 points in Fig. \ref{fig:representation}(a) first row. But the 4 points on the image boundaries help little when combine different views, because they are not real corners. 
Moreover, in order to determine how many corners to select in the final layout, a classification branch is required to determine the layout topology. 
Out of this reason, we propose a more concise representation of a room layout using the corners only, as shown in Fig. \ref{fig:representation}(d). 
This representation avoids extra efforts on classifying the room layout topology, and can be easily used to integrate multiple views.
%


%When the overall FOV of the multi-view images contains the entire room, we can recover the whole-room layout, better viewed in panorama, Fig. \ref{fig:results2}. 
%

%\xj{then pipeline.}
Given multiple overlapping images of an indoor scene from different views, our approach produces the combined room layout in corner-based representation. The pipeline is depicted in Fig. \ref{fig:overview}. 
%
First, we estimate the partial room layout separately in each single image using an encoder-decoder structure, as described in Sec. \ref{sec:layout}. 
Then the estimated partial room layouts are integrated into a combined heatmap, based on the geometric transformations between images. 
We use a linear blending to average the predictions from different views, and select the points of the maximum responses as the room corners, as described in Sec. \ref{sec:merging}. 

 



\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/representation.png}
	\caption{Two different room types and our new representation. The first three columns are clipped from LSUN specification \cite{zhang2015large}. }
	\label{fig:representation}
\end{figure}

\subsection{Single-view Layout Estimation}
\label{sec:layout}

%
%A direct way to achieve our goal is to utilize existing methods to estimate the layout for each view, and then combine these representations to produce the complete-room 3D layout. 
%However, this method is not efficient due to many redundant predictions caused by overlaps across views. 
\comments{
To avoid redundant predictions or even to benefit from them, we propose a \emph{secondary representation} of a room layout on perspective images. The room layout under each view is only represented by the intersections of vertical walls and ceiling or floor. We call these intersections secondary keypoints. 
\xj{What about the case when there in only one wall surface?}
Obviously, without the extra intersections of two semantic planes on the image boundaries, we cannot recover the room layout from a single perspective. However, these secondary keypoints in overlapping views are sufficient to reconstruct the 3D layout of the entire room. Compared to previous representations, our secondary keypoints are quite simplified and easier to train. It also naturally avoids a lot of redundant predictions. 
}

%\comments{ motivated by \cite{LeeRoomNet17} and the studies on human pose estimation \cite{tompson2014joint,pfister2015flowing}, we formulate the room corners on the image plane as 2D Gaussian centered at their locations. The standard deviation is set to 40 pixels in our case. \xj{this format is for training, right?}
%

%In this section, we are going to recover the room layout for perspective images from different views. 
%
%Compared to previous representations, our corner-based representation is quite simplified and easier to train. 
%It also naturally avoids a lot of redundant predictions. 
%\xj{remove all "secondary representation".}
 
 
Under our corner-based layout representation, we predict the corner positions for each view separately using a neural network. 
Considering the inherent semantic differences, we divide the room corners into two categories: the upper corners which are the intersection of two walls and ceiling, and the lower corners which are the intersection of two walls and floor. 
In consequence, the goal of our single-view network should be estimating the heatmaps of these two types of room corners for the input image.

\noindent\textbf{Network Architecture.}
We adopt the encoder-decoder architecture proposed by \cite{LeeRoomNet17} with modifications, since it is an end-to-end framework without complex post-processing. 
%
Our layout estimation network for a single-view image is shown in Fig.~\ref{fig:network}. 
%It is designed to estimate the room corners in a single-view image. 
%Our network for room corner estimation from a single-view image is shown in Fig.~\ref{fig:network}.
We mainly make two modifications on the decoder part and classification branch. 
First, as our corner-based representation is more concise, we remove the recurrent structure in \cite{LeeRoomNet17} for efficiency, and we add more upsampling layers and convolution layers to upsample the feature maps from the bottleneck layer to get the same resolution with the input image as a compensation for accuracy. 
The final convolution layer is adapted to output a $w \times h \times 2$ probability array $T$ for our new representation, where $w$ and $h$ stand for the width and length of the input image. 
Each of the two slices can be viewed as a heatmap for the upper corners and lower corners respectively. 
Secondly, \cite{LeeRoomNet17} delineates room layout using 2D keypoints. The semantic boundaries can be recovered by connecting the detected 2D keypoints in a specific order. 
As the connection order differs between room topologies, they have to add a classification branch in their network to classify the room type, and the final performance rely on the classification accuracy. 
Unfortunately, the accuracy is not satisfied. 
We naturally avoid this situation by using our representation. The simplified representation divides the room corners into only two categories and applies to all room types. 
Therefore, we remove the classification branch in our network architecture.
 
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/network.png}
	\caption{Network architecture. The encoder part is topologically identical to the VGG16 network. It encodes the $320 \times 320$ input images to $10 \times 10$ feature maps. Then the decoder part upsample the feature maps to full input resolution. }
	\label{fig:network}
\end{figure}

\noindent\textbf{Training.} 
While our method could take multiple images under different view directions of arbitrary FOV, we use the PanoContext dataset \cite{zhang2014panocontext} to generate multi-view images for training. 
The PanoContext dataset contains 500 annotated panoramas. We project these panoramas horizontally into multiple $320 \times 320$ images with $FOV=\ang{90}$ at different views using the toolkit provided by \cite{zhang2014panocontext}. For each panorama we obtain 12 perspective images.
%
%\xj{The number of views in the training? how many training images? with overlapping? data augumentation?} \drf{we use different views for different settings (in the experiments), so i didn't mention it here.}

The ground truth locations of the room corners are obtained using the same transformations and are further transformed into the Gausian representation. 
While the output is a heatmap, we produce the ground-truth map for each image by generating a 2D Gaussian centered at the location of each groundtruth corner with $\sigma=40$ pixels, similar with \cite{LeeRoomNet17,tompson2014joint}.
%
Euclidean loss is used to regress the heatmap of room corners. 
Because the background area is much larger than the corner areas in the heatmaps, we adjust the distribution imbalance between foreground and background pixels by degrading the gradient weight of background pixels with a coefficient of 0.2.
%\xj{Training parameters?}
Adam is adopted as the optimizer to train the model with batch size of 5, initial learning rate of 0.00001 and maximum iterations is set to 40000.

%To obtain multiple overlapping perspective images, we project the panoramic images into different views using the toolkit provided by \cite{pano}. The ground truth of the secondary keypoints is represented by several 2D Gaussian heatmaps centered at their locations. We adjust the distribution imbalance between foreground and background pixels by degrading the gradient weight of background pixels with a coefficient of 0.2.

%The RoomNet-basic struture in \cite{RoomNet} is adopted in our training stage for efficiency. We first pretrain the Network on LSUN \cite{LSUN2016} training set. Then, to finetune the model on images from different views in the same room, we project the panorama from \cite{PanoContext} to $k$ views. We set $k$ to 12 and 24 in our experiment. The layout ground truth is relabeled using the same projection. 

%The secondary keypoints can be divided into two categories according to semantics: the intersection of two walls and ceiling or the intersection of two walls and floor.\xj{Move this sentence to the representation paragraph.}

%We train the network to regress these two kinds of keypoints separately in order to eliminate the ambiguity between them. For this reason, the output of our network is a $w \times h \times 2$ probability array $T$, where $w$ and $h$ stand for the width and length of the input image. Each of the 2 slices can be viewed as a probability map for the secondary keypoints in a corresponding category. 


\subsection{Layout Composition}
\label{sec:merging}
After we predict the heatmaps of the room corners for each image, we composite them to generate a layout in a larger FOV. 
First, we align the input images in different views by using the transformation between image plane and the composition image plane. 
%First, the input multi-view images are projected into a combined image using the camera poses, we formulate this view transformations as a function $f$. 
Then, we sum the two-channel heatmap $T$ for each image to get a heatmap $\hat{T}$ depicting all two types of room corners. 
Next, we use the same transformation to map the heatmaps $\hat{T}$ from multi-views into the composition image plane.
%
%\xj{Explain the different between two subfigures. }
The combined heatmap generated by simply averaging the maps of different views contains many visual boundaries as the data distribution differs in different views. To smooth the combined heatmap, a linear blending is applied to average the predictions of the overlapping area between different views. The weight for blending is depended on the distance to each overlapping images for each pixel.

%
After that, we pick the points with the local maximum responses as the room corners. 
At this stage, we first reduce the noise in $\hat{T}$ caused by unsatisfactory predictions from particular perspectives by calculating the LOG response of $\hat{T}$ at a certain scale, $\sigma$ is set to 21 in our case, and use the LOG response as our new heatmap.
%
Then, we follow the post-processing method in \cite{zou2018layoutnet} to obtain the locations of the room corners in the combined map. In brief, the combined heatmap is summed across rows to find $m$ local maxima for columns, then $n$ largest peaks are found along each of the $m$ columns. In this way, we attain the locations of $m \times n$ room corners. We further detect the vanishing points of the two perspective images on two sides of the combined image, and connect them seperately with the closet room corners in the combined layout to generate the semantic boundaries outside the $m \times n$ room corners. While for panorama, we reconstruct the whole-room layout by by directly connecting the eight keypoints.
%

\comments{
\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/blending.png}
	\caption{The combined probability map. Using simply average (left) or linear blending along horizontal lines (right).}
	\label{fig:blending}
\end{figure}
}

\comments{
\subsection{Alignment and 3D Reconstruction}
\label{sec:align}

(Optional and undone) In this section, we align the panoramic images to make sure that wall-wall boundaries are vertical to the floor. If we use the panorama to generate testing images, this step can be omitted as the reprojected panoramic images naturally met this alignment condition. Then the aligned panorama can be further rendered into a 3D representation. These two steps are implemented using existing techniques but the rendering part is not yet available. 
}
